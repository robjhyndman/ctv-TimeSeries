<!--Hey Emacs, make this -*- mode: XML -*- -->
<CRANTaskView>

   <name>HighPerformanceComputing</name>
   <topic>High-Performance and Parallel Computing with R</topic>
   <maintainer email="Dirk.Eddelbuettel@R-project.org">Dirk Eddelbuettel</maintainer>
   <version>2018-01-13</version>

  <info>
    <p>
      This CRAN task view contains a list of packages, grouped by topic, that
      are useful for high-performance computing (HPC) with R.  In this context, we
      are defining 'high-performance computing' rather loosely as just about anything
      related to pushing R a little further: using compiled code,
      parallel computing (in both explicit and implicit modes), working with
      large objects as well as profiling. 
    </p>
    <p>
      Unless otherwise mentioned, all packages presented with hyperlinks 
      are available from CRAN, the
      Comprehensive R Archive Network. 
    </p>
    <p>
      Several of the areas discussed in this Task View are undergoing rapid
      change. Please send suggestions for additions and extensions for this task
      view to the <a href="mailto:Dirk.Eddelbuettel@R-project.org">task view maintainer</a>.
    </p>
    <p>
      Suggestions and corrections by Achim Zeileis, Markus
      Schmidberger, Martin Morgan, Max Kuhn, Tomas Radivoyevitch,
      Jochen Knaus, Tobias Verbeke, Hao Yu, David Rosenberg, Marco
      Enea, Ivo Welch, Jay Emerson, Wei-Chen Chen, Bill Cleveland, 
      Ross Boylan, Ramon Diaz-Uriarte, Mark Zeligman, Kevin Ushey, Graham
      Jeffries, and Will Landau (as well as others I may have forgotten to
      add here) are gratefully acknowledged.  
    </p>
    <p> 
      Contributions are always welcome, and encouraged. Since the start of
      this CRAN task view in October 2008, most contributions have arrived as
      email suggestions. The source file for this particular task view file
      now also reside in a GitHub repository (see below) so that pull
      requests are also possible. 
    </p>
    <p>
      The <code>ctv</code> package supports these Task Views. Its functions
      <code>install.views</code> and <code>update.views</code> allow,
      respectively, installation or update of packages from a given Task View;
      the option <code>coreOnly</code> can restrict operations to packages labeled as
      <em>core</em> below.
    </p>
    <p>
      <strong>Direct support in R started with release 2.14.0</strong> 
      which includes a new package <strong>parallel</strong> incorporating 
      (slightly revised) copies of packages multicore and 
      <pkg>snow</pkg>.  Some types of clusters are not handled directly by 
      the base package 'parallel'. However, and as explained in the package vignette, the parts of
      parallel which provide <pkg>snow</pkg>-like functions will
      accept <pkg>snow</pkg> clusters including MPI clusters. 
      <br/>

      The <strong>parallel</strong> package also contains support for multiple 
      RNG streams following L'Ecuyer et al (2002), with support for 
      both mclapply and snow clusters.  
      <br/>

      The version released for R 2.14.0 contains base functionality:
      higher-level convenience functions are planned for later R releases.
    </p>

    <p>
      <strong>Parallel computing: Explicit parallelism</strong>
    </p>
    <ul>	     
      <li>Several packages provide the communications layer required for parallel
	computing. The first package in this area was
	rpvm by Li and Rossini which uses the PVM (Parallel Virtual
	Machine) standard and libraries. rpvm is no longer actively
	maintained, but available from its CRAN archive directory.
      </li>
      <li>In recent years, the
        alternative MPI (Message Passing Interface) standard has become the
        de facto standard in parallel computing. It is supported in R via
  	the <pkg>Rmpi</pkg> by Yu. <pkg>Rmpi</pkg> package is mature yet actively
	maintained and offers access to numerous functions from the MPI
	API, as well as a number of R-specific extensions.  <pkg>Rmpi</pkg>
        can be used with the LAM/MPI, MPICH / MPICH2, Open MPI, and Deino MPI 
        implementations. It should be noted that LAM/MPI is now in
        maintenance mode, and new development is focused on Open MPI.
      </li>
      <li>
        The <pkg>pbdMPI</pkg> package provides S4 classes to directly interface 
        MPI in order to support the Single Program/Multiple Data (SPMD) parallel 
        programming style which is particularly useful for batch parallel execution.
	The <pkg>pbdSLAP</pkg> builds on this and uses scalable linear algebra 
	packages (namely BLACS, PBLAS, and ScaLAPACK) in double precision based 
	on ScaLAPACK version 2.0.2. 
	The <pkg>pbdBASE</pkg> builds on these and provides the core classes
        and methods for distributed data types upon which the 
	<pkg>pbdDMAT</pkg> builds to provide distributed dense matrices for "Programming 
	with Big Data". The <pkg>pbdNCDF4</pkg> package permits
	multiple processes to write to the same file (without manual
	synchronization) and supports terabyte-sized files.
	The <pkg>pbdDEMO</pkg> package provides examples for these
	packages, and a detailed vignette.
        The <pkg>pbdPROF</pkg> package profiles MPI communication SPMD code
        via MPI profiling libraries, such as fpmpi, mpiP, or TAU.
      </li>
      <li>An alternative is provided by the <pkg>nws</pkg> (NetWorkSpaces)
        packages from REvolution Computing. It is the successor to the
	earlier LindaSpaces approach to parallel computing, and is
	implemented on top of the Twisted networking toolkit for Python.
      </li>
      <li>The <pkg>snow</pkg> (Simple Network of Workstations) package by
        Tierney et al. can use PVM, MPI, NWS as well as direct networking
        sockets. It provides an abstraction layer by hiding the
	communications details. The <pkg>snowFT</pkg> package provides 
        fault-tolerance extensions to <pkg>snow</pkg>.
      </li>
      <li>The <pkg>snowfall</pkg> package by Knaus provides a more recent
	alternative to <pkg>snow</pkg>. Functions can be used in sequential or
	parallel mode.
      </li>
      <li>The <pkg>foreach</pkg> package allows general iteration over
	elements in a collection without the use of an explicit loop
	counter. Using foreach without side effects also facilitates
	executing the loop in parallel which is possible via
	the <pkg>doMC</pkg> (using parallel/multicore on single
	workstations), <pkg>doSNOW</pkg> (using <pkg>snow</pkg>, see
	above), <pkg>doMPI</pkg> (using <pkg>Rmpi</pkg>) packages,
        <pkg>doFuture</pkg> (using <pkg>future</pkg> or
        <pkg>future.BatchJobs</pkg>), and
	<pkg>doRedis</pkg> (using <pkg>rredis</pkg>) packages.
      </li>
      <li>The <pkg>future</pkg> package allows for synchronous (sequential)
        and asynchronous (parallel) evaluations via abstraction of futures,
        either via function calls or implicitly via promises. Global variables
        are automatically identified. Iteration over elements in a collection
        is supported.
      </li>
      <li>The <pkg>Rborist</pkg> package employs OpenMP pragmas to exploit 
        predictor-level parallelism in the Random Forest algorithm which
        promotes efficient use of multicore hardware in restaging data and in 
        determining splitting criteria, both of which are performance 
        bottlenecks in the algorithm.
      </li>
      <li>The <pkg>h2o</pkg> package connects to the h2o open source machine
        learning environment which has scalable implementations of random
        forests, GBM, GLM (with elastic net regularization), and deep learning.
      </li>
      <li>The <pkg>randomForestSRC</pkg> package can use both OpenMP as well
        as MPI for random forest extensions suitable for survival analysis,
        competing risks analysis, classification as well as regression 
      </li>
      <li>The <pkg>parSim</pkg> package can perform simulation studies using
        one or multiple cores, both locally and on HPC clusters.
      </li>

    </ul>

    <p>
    <strong>Parallel computing: Implicit parallelism</strong>
    </p>
    <ul>	     
      <li>The pnmath package by Tierney 
	(<a href="http://www.stat.uiowa.edu/~luke/R/experimental/">link</a>)
	uses the Open MP parallel processing directives of recent compilers
        (such gcc 4.2 or later) for implicit parallelism by replacing a
        number of internal R functions with replacements that can make use of
        multiple cores --- without any explicit requests from the user.  The
        alternate pnmath0 package offers the same functionality using
        Pthreads for environments in which the newer compilers are not
        available.  Similar functionality is expected to become integrated
        into R 'eventually'.
      </li>
      <li>The romp package by Jamitzky was presented at useR! 2008
        (<a href="http://www.statistik.tu-dortmund.de/useR-2008/slides/Jamitzky.pdf">slides</a>)
        and offers another interface to Open MP using Fortran. The code is still
	pre-alpha and available from the Google Code project <gcode>romp</gcode>.
	An R-Forge project <rforge>romp</rforge> was initiated but there is no package, yet.
      </li>
      <li>The R/parallel package by Vera, Jansen and Suppi offers a C++-based master-slave dispatch
        mechanism for parallel execution (<a href="http://www.rparallel.org/">link</a>)
      </li>
      <li>The <pkg>Rdsm</pkg> package provides a threads-like parallel
	computing environment, both on multicore machine and across the network
	by providing facilities inspired from distributed shared memory
	programming.
      </li>
      <li>
        The <pkg>RhpcBLASctl</pkg> package detects the number of available
        BLAS cores, and permits explicit selection of the number of
        cores.
      </li>
      <li>
        The <pkg>Rhpc</pkg> permits <code>*apply()</code> style dispatch via MPI.
      </li>
      <li>
        The <pkg>drake</pkg> package is an R-focused 
        <a href="https://github.com/pditommaso/awesome-pipeline">pipeline toolkit</a> similar to
        <a href="https://www.gnu.org/software/make">Make</a>. Parallel computing relies on the
        <code>parallel</code>, <code>future</code>, <code>batchtools</code>,
        and <code>future.batchtools</code> packages, as well as 
        <a href="https://www.gnu.org/software/make">Makefiles</a>. Drake uses code analysis
        to configure the user's workflow and make the parallelism implicit.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Grid computing</strong>
    </p>
    <ul>	     
      <li>The multiR package by Grose was presented at useR! 2008 
	but has not been released. It may offer a snow-style framework on a grid computing platform.
      </li>
      <li>The <rforge>biocep-distrib</rforge> project by Chine offers a Java-based framework for local, Grid,
        or Cloud computing. It is under active development.
      </li>
    </ul>


    <p>
    <strong>Parallel computing: Hadoop</strong>
    </p>
    <ul>	     
      <li>
	The RHIPE package, started by Saptarshi Guha and now developed by a core team via
	<a href="https://github.com/saptarshiguha/RHIPE/">GitHub</a>,   
        provides an interface between R and Hadoop for analysis of large complex data wholly from 
	within R using the Divide and Recombine approach to big data.
      </li>
      <li>
	The rmr package by Revolution Analytics also provides an interface between R and Hadoop
	for a Map/Reduce programming framework. (<a href="https://github.com/RevolutionAnalytics/RHadoop/wiki/rmr">link</a>)
      </li>
      <li>
	A related package, segue package by Long, permits easy execution of embarassingly parallel task on Elastic Map Reduce (EMR) at Amazon.
	(<a href="http://code.google.com/p/segue/">link</a>)
      </li>
      <li>
        The <pkg>RProtoBuf</pkg> package provides an interface to
        Google's language-neutral, platform-neutral, extensible
        mechanism for serializing structured data.  This package can
        be used in R code to read data streams from other systems in a
        distributed MapReduce setting where data is serialized and
        passed back and forth between tasks.
      </li>
      <li>
        The <pkg>HistogramTools</pkg> package provides a number of
        routines useful for the construction, aggregation,
        manipulation, and plotting of large numbers of Histograms such
        as those created by Mappers in a MapReduce application.
      </li>
      <li>
        The <pkg>toaster</pkg> package performs in-database computations
        utilizing the parallel / distributed Teradata Aster analytical platform
      </li>
    </ul>	     

    <p>
    <strong>Parallel computing: Random numbers</strong>
    </p>
    <ul>	     
      <li>Random-number generators for parallel computing are available via
        the <pkg>rlecuyer</pkg>
        package by Sevcikova and Rossini.
      </li>

      <li>The <pkg>doRNG</pkg> package provides functions to perform
        reproducible parallel foreach loops, using independent random
        streams as generated by the package rstream, suitable for the
        different foreach backends.
      </li>
    </ul>



    <p>
    <strong>Parallel computing: Resource managers and batch schedulers</strong>
    </p>
    <ul>	     
       <li>
        Job-scheduling toolkits permit management of
	parallel computing resources and tasks.  The slurm (Simple Linux
        Utility for Resource Management) set of programs  works well with
        MPI and slurm jobs can be submitted from R using the <pkg>rslurm</pkg> 
        package. (<a href="http://slurm.schedmd.com/">link</a>)
      </li>
      <li>
        The Condor toolkit (<a href="http://www.cs.wisc.edu/condor/">link</a>) from 
	the University of Wisconsin-Madison has been used with R as described
	in this <a href="http://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf">R
        News article</a>.
      </li>
      <li>
        The sfCluster package by Knaus can be used with <pkg>snowfall</pkg>.
	(<a href="http://www.imbi.uni-freiburg.de/parallel/">link</a>) but is
	currently limited to LAM/MPI. 
      </li>
      <li>
        The <pkg>batch</pkg> package by Hoffmann can launch parallel computing
        requests onto a cluster and gather results.
      </li>
      <li>
	The <pkg>BatchJobs</pkg> package provides Map, Reduce and
	Filter variants to manage R jobs and their results on batch
	computing systems like PBS/Torque, LSF and Sun Grid
	Engine. Multicore and SSH systems are also supported. The
	<pkg>BatchExperiments</pkg> package extends it with an
	abstraction layer for running statistical experiments. Package
        <pkg>batchtools</pkg> is a successor / extension to both.
      </li>
      <li>
        The <pkg>flowr</pkg> package offers a scatter-gather approach to submit jobs
        lists (including dependencies) to the computing cluster via simple data.frames
        as inputs. It supports LSF, SGE, Torque and SLURM.
      </li>
      <li>
        The <pkg>clustermq</pkg> package sends function calls as jobs on LSF,
        SGE and SLURM via a single line of code without using network-mounted
        storage. It also supports use of remote clusters via SSH.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Applications</strong>
    </p>
    <ul>	     
      <li>
        The <pkg>caret</pkg> package by Kuhn can use various frameworks
	(MPI, NWS etc) to parallelized cross-validation and bootstrap characterizations of predictive models.
      </li>
      <li>
	The <bioc>maanova</bioc> package on Bioconductor by Wu can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for the analysis of micro-array experiments.
      </li>
      <li>
	The <pkg>pvclust</pkg> package by Suzuki and Shimodaira can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for hierarchical clustering via multiscale
	bootstraps.
      </li>
      <li>
	The <pkg>tm</pkg> package by Feinerer can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for parallelized text mining.
      </li>
      <li>
	The <pkg>varSelRF</pkg> package by Diaz-Uriarte can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for parallelized use of variable selection via
	random forests.
      </li>
      <li>
	The <pkg>bcp</pkg> package by Erdman and Emerson for the Bayesian
	analysis of change points can use <pkg>foreach</pkg> for parallelized operations.
      </li>
      <li>
	The <bioc>multtest</bioc> package by Pollard et al. on Bioconductor can
	use <pkg>snow</pkg>, <pkg>Rmpi</pkg> or rpvm for
	resampling-based testing of multiple hypothesis.
      </li>
      <li>
	The <pkg>GAMBoost</pkg> package by Binder for <code>glm</code>
	and <code>gam</code> model fitting via boosting using b-splines,
	the <pkg>Matching</pkg> package by Sekhon for multivariate and propensity
	score matching,
	the <pkg>STAR</pkg> package by Pouzat for spike train analysis,
	the <pkg>bnlearn</pkg> package by Scutari for bayesian network
	structure learning,
	the <pkg>latentnet</pkg> package by Krivitsky and Handcock for latent
	position and cluster models,
	the <pkg>lga</pkg> package by Harrington for linear grouping analysis,
	the <pkg>peperr</pkg> package by Porzelius and Binder for parallelised
	estimation of prediction error,
	the <pkg>orloca</pkg> package by Fernandez-Palacin and Munoz-Marquez
	for operations research locational analysis,
	the <pkg>rgenoud</pkg> package by Mebane and Sekhon for genetic
	optimization using derivatives 
	the <bioc>affyPara</bioc> package by Schmidberger, Vicedo and
	Mansmann for parallel normalization of Affymetrix microarrays, 
	and the <bioc>puma</bioc> package by Pearson et al. which propagates
	uncertainty into standard microarray analyses such as differential
	expression
	all can use <pkg>snow</pkg> for parallelized operations using either
	one of the MPI, PVM, NWS or socket protocols supported by <pkg>snow</pkg>.
      </li>
      <li>The <gcode>bugsparallel</gcode> package uses <pkg>Rmpi</pkg> for distributed
          computing of multiple MCMC chains using WinBUGS. 
      </li>
      <li>The <pkg>xgboost</pkg> package by Chen et al. is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. The same code runs on major distributed environment, such as Hadoop, SGE, and MPI.
      </li>
      <li>The <pkg>partDSA</pkg> package uses <pkg>nws</pkg> for generating a
          piecewise constant estimation list of increasingly complex
          predictors based on an intensive and comprehensive search over the
          entire covariate space.
      </li>
      <li>The <pkg>dclone</pkg> package provides a global optimization
	approach and a variant of simulated annealing which exploits Bayesian
	MCMC tools to get MLE point estimates and standard errors using low
	level functions for implementing maximum likelihood estimating
	procedures for complex models using data cloning and Bayesian Markov
	chain Monte Carlo methods with support for JAGS, WinBUGS and
	OpenBUGS; parallel computing is supported via the <pkg>snow</pkg>
	package.
      </li>
      <li>The <pkg>pmclust</pkg> package utilizes unsupervised model-based
        clustering for high dimensional (ultra) large data. The package uses
	<pkg>pbdMPI</pkg> to perform a parallel version of the EM algorithm for
	finite mixture Gaussian models.
      </li>
      <li>The <pkg>harvestr</pkg> package provides helper functions
        for (reproducible) simulations.
      </li>
      <li>
        Nowadays, many packages can use the facilities offered by
        the <strong>parallel</strong> package. One example 
        is <pkg>pls</pkg>, another is <pkg>PGICA</pkg> which can run ICA
        analysis in parallel on SGE or multicore platforms.
      </li>
      <li>
        The <pkg>sprint</pkg> (an acronym for "Simple Parallel R INTerface")
        package provides a parallel computing framework for R making High
        Performance Computing (HPC) accessible to users who are not familiar
        with parallel programming and the use of HPC architectures. It
        contains a library of parallelised R functions for correlation,
        partitioning around medoids, apply, permutation testing,
        bootstrapping, random forest, rank product and hamming distance. 
      </li>
      <li>
        The <pkg>pbapply</pkg> package offers a progress bar for vectorized R
        functions in the `*apply` family, and supports several backends.
      </li>
      <li>
        The <pkg>Sim.DiffProc</pkg> package simulates and estimates
        multidimensional It√¥ and Stratonovich stochastic differential
        equations in parallel.
      </li>
      <li>
        The <pkg>keras</pkg> package by by Allaire et al. provides a
        high-level neural networks API. It was developed with a focus
        on enabling fast experimentation for convolutional networks,
        recurrent networks, any combination of both, and custom neural
        network architectures.
      </li>
        
    </ul>

    <p>
    <strong>Parallel computing: GPUs</strong>
    </p>
    <ul>
      <li>
        The <pkg>cudaBayesreg</pkg> package by da Silva implements
        the <code>rhierLinearModel</code> from the <pkg>bayesm</pkg> package 
	using nVidia's CUDA language and tools to provide high-performance
        statistical analysis of fMRI voxels.
      </li>
      <li>
        The rgpu package (see below for link) aims to speed up bioinformatics
        analysis by using the GPU.
      </li>
      <li>
	The <pkg>gcbd</pkg> package implements a benchmarking framework for
	BLAS and GPUs.
      </li>
      <li>
	The <pkg>OpenCL</pkg> package provides an interface from R to
	OpenCL permitting hardware- and vendor neutral interfaces to
	GPU programming.
      </li>
      <li>
        The <pkg>permGPU</pkg> package computes permutation resampling
        inference in the context of RNA microarray studies on the GPU,
        it uses CUDA (>= 4.5)
      </li>
      <li>
        The <pkg>gpuR</pkg> package offers GPU-enabled functions: New gpu*
        and vcl* classes are provided to wrap typical R objects (e.g. vector,
        matrix) mirroring typical R syntax without the need to know OpenCL. 
      </li>
      <li>
        The <pkg>tensorflow</pkg> package by by Allaire et
        al. provides access to the complete TensorFlow API from within
        R that enables numerical computation using data flow
        graphs. The flexible architecture allows users to deploy
        computation to one or more CPUs or GPUs in a desktop, server,
        or mobile device with a single API.
      </li>
      <li>
        The <pkg>tfestimators</pkg> package by by Tang et al. offers a
        high-level API that provides implementations of many different
        model types including linear models and deep neural
        networks. It also provides a flexible framework for defining
        arbitrary new model types as custom estimators with the
        distributed power of TensorFlow for free.
      </li>
    </ul>

    <p>
    <strong>Large memory and out-of-memory data</strong>
    </p>
    <ul>
      <li>The <pkg>biglm</pkg> package by Lumley uses incremental computations to
        offer <code>lm()</code> and <code>glm()</code> functionality to 
        data sets stored outside of R's main memory.
      </li>
      <li>The <pkg>ff</pkg> package by Adler et al. offers file-based access to data sets
        that are too large to be loaded into memory, along with a number of
        higher-level functions.
      </li>
      <li>The <pkg>bigmemory</pkg> package by Kane and Emerson permits storing large objects such
        as matrices in memory (as well as via files) and uses external
        pointer objects to refer to them.  This permits transparent access
        from R without bumping against R's internal memory limits.  Several R
        processes on the same computer can also share big memory objects.
      </li>
      <li>A large number of database packages, and database-alike packages
        (such as <pkg>sqldf</pkg> by Grothendieck and <pkg>data.table</pkg>
        by Dowle) are also of potential interest but not reviewed here.
      </li>
      <li>The <pkg>HadoopStreaming</pkg> package provides a framework for
	writing map/reduce scripts for use in Hadoop Streaming; it also
	facilitates operating on data in a streaming fashion which does not
	require Hadoop.
      </li>
      <li>The <pkg>speedglm</pkg> package permits to fit (generalised) linear
	models to large data.  For in-memory data sets, speedlm() or
	speedglm() can be used along with update.speedlm() which can update
	fitted models with new data. For out-of-memory data sets, shglm() is
	available; it works in the presence of factors and can check for
	singular matrices.
      </li>
      <li>
	The <pkg>biglars</pkg> package by Seligman et al can use the <pkg>ff</pkg>
	to support large-than-memory datasets for least-angle regression,
	lasso and stepwise regression.
      </li>
      <li>The <pkg>MonetDB.R</pkg> package allows R to access the MonetDB 
         column-oriented, open source database system as a
	 backend. 
      </li>
      <li>The <pkg>ffbase</pkg> package by de Jonge et al adds basic
        statistical functionality to the <pkg>ff</pkg> package.
      </li>
      <li>The <pkg>LaF</pkg> package provides methods for fast access to
        large ASCII files in csv or fixed-width format.
      </li>
    </ul>

    <p>
    <strong>Easier interfaces for Compiled code</strong>
    </p>
    <ul>
      <li>
	The <pkg>inline</pkg> package by Sklyar et al eases adding code in C,
	C++ or Fortran to R. It takes care of the compilation, linking and
	loading of embedded code segments that are stored as R strings.
      </li>
      <li>
	The <pkg>Rcpp</pkg> package by Eddelbuettel and Francois offers a
        number of C++ classes that makes transferring R objects to C++
        functions (and back) easier, and the <pkg>RInside</pkg> package
        by the same authors allows easy embedding of R itself into C++
        applications for faster and more direct data transfer.
      </li>
      <li>
        The <pkg>RcppParallel</pkg> package by Allaire et al. bundles the
        <a href="https://www.threadingbuildingblocks.org">Intel Threading Building Blocks</a>
        and <a href="http://tinythreadpp.bitsnbites.eu">TinyThread</a>
        libraries. Together with <pkg>Rcpp</pkg>, RcppParallel makes it
        easy to write safe, performant, concurrently-executing C++ code,
        and use that code within R and R packages.
      </li>
      <li>
	The <pkg>rJava</pkg> package by Urbanek provides a low-level
	interface to Java similar to the <code>.Call()</code> interface for C
	and C++.
      </li>
      <li>
	The <pkg>reticulate</pkg> package by Allaire provides interface to Python modules, classes, and functions. It allows R users to access many high-performance Python packages such as <pkg>tensorflow</pkg> and <pkg>tfestimators</pkg> within R.
      </li>
      <!-- <li>Fortran interfaces -->
      <!-- </li> -->
      <!-- <li> -->
      <!-- </li>Debugging tools -->
    </ul>

    <p>
    <strong>Profiling tools</strong>
    </p>
    <ul>
      <li>The <pkg>profr</pkg> package by Wickham can visualize output from
	the <code>Rprof</code> interface for profiling.
      </li>
      <li>The <pkg>proftools</pkg> package by Tierney, and the
        <pkg>aprof</pkg> package by Visser, can also be used to analyse
        profiling output. 
      </li>
      <li> The <pkg>GUIProfiler</pkg> package visualizes the results of
        profiling R programs.
      </li>

    </ul>

  </info>

  <packagelist>
    <pkg>aprof</pkg>
    <pkg>batch</pkg>
    <pkg>BatchExperiments</pkg>
    <pkg>BatchJobs</pkg>
    <pkg>batchtools</pkg>
    <pkg>bayesm</pkg>
    <pkg>bcp</pkg>
    <pkg>biglars</pkg>
    <pkg>biglm</pkg>
    <pkg>bigmemory</pkg>
    <pkg>bnlearn</pkg>
    <pkg>caret</pkg>
    <pkg>clustermq</pkg>
    <pkg>cudaBayesreg</pkg>
    <pkg>data.table</pkg>
    <pkg>dclone</pkg>
    <pkg>doFuture</pkg>
    <pkg>doMC</pkg>
    <pkg>doMPI</pkg>
    <pkg>doRedis</pkg>
    <pkg>doRNG</pkg>
    <pkg>doSNOW</pkg>
    <pkg>drake</pkg>
    <pkg>ff</pkg>
    <pkg>ffbase</pkg>
    <pkg>flowr</pkg>
    <pkg>foreach</pkg>
    <pkg>future</pkg>
    <pkg>future.BatchJobs</pkg>
    <pkg>GAMBoost</pkg>
    <pkg>gcbd</pkg>
    <pkg>gpuR</pkg>
    <pkg>GUIProfiler</pkg>
    <pkg>h2o</pkg>
    <pkg>HadoopStreaming</pkg>
    <pkg>harvestr</pkg>
    <pkg>HistogramTools</pkg>
    <pkg>inline</pkg>
    <pkg>keras</pkg>
    <pkg>LaF</pkg>
    <pkg>latentnet</pkg>
    <pkg>lga</pkg>
    <pkg>Matching</pkg>
    <pkg>MonetDB.R</pkg>
    <pkg>nws</pkg>
    <pkg>orloca</pkg>
    <pkg>OpenCL</pkg>
    <pkg>parSim</pkg>
    <pkg>partDSA</pkg>
    <pkg>pbapply</pkg>    
    <pkg>pbdBASE</pkg>
    <pkg>pbdDEMO</pkg>
    <pkg>pbdDMAT</pkg>
    <pkg>pbdPROF</pkg>
    <pkg>pbdMPI</pkg>
    <pkg>pbdNCDF4</pkg>
    <pkg>pbdSLAP</pkg>
    <pkg>peperr</pkg>
    <pkg>permGPU</pkg>
    <pkg>PGICA</pkg>
    <pkg>pls</pkg>
    <pkg>pmclust</pkg>
    <pkg>profr</pkg>
    <pkg>proftools</pkg>
    <pkg>pvclust</pkg>
    <pkg>randomForestSRC</pkg>    
    <pkg>Rborist</pkg>
    <pkg>Rcpp</pkg>
    <pkg>RcppParallel</pkg>
    <pkg>Rdsm</pkg>
    <pkg>reticulate</pkg>
    <pkg>rgenoud</pkg>
    <pkg>Rhpc</pkg>
    <pkg>RhpcBLASctl</pkg>
    <pkg>RInside</pkg>
    <pkg>rJava</pkg>
    <pkg>rlecuyer</pkg>
    <pkg priority="core">Rmpi</pkg>
    <pkg>RProtoBuf</pkg>
    <pkg>rredis</pkg>
    <pkg>Sim.DiffProc</pkg>
    <pkg>rslurm</pkg>
    <pkg priority="core">snow</pkg>
    <pkg>snowfall</pkg>
    <pkg>snowFT</pkg>
    <pkg>speedglm</pkg>
    <pkg>sprint</pkg>    
    <pkg>sqldf</pkg>
    <pkg>STAR</pkg>
    <pkg>tensorflow</pkg>
    <pkg>tfestimators</pkg>
    <pkg>tm</pkg>
    <pkg>toaster</pkg>
    <pkg>varSelRF</pkg>
    <pkg>xgboost</pkg>
  </packagelist>

  <links>
    <a href="http://www.stat.uiowa.edu/~luke/classes/295-hpc/">HPC computing notes by Luke Tierney for HPC class at University of Iowa</a> 
    <a href="https://stat.ethz.ch/mailman/listinfo/r-sig-hpc/">Mailing List: R Special Interest Group High Performance Computing</a>
    <a href="http://www.jstatsoft.org/v31/i01/">Schmidberger, Morgan, Eddelbuettel, Yu, Tierney and Mansmann (2009) paper on 'State of the Art in Parallel Computing with R'</a>
    <a href="http://www.stat.uiowa.edu/~luke/R/experimental/">Luke Tierney's code directory for pnmath and pnmath0</a>
    <rforge>biocep-distrib</rforge>
    <bioc>affyPara</bioc>
    <bioc>maanova</bioc>
    <bioc>multtest</bioc>
    <bioc>puma</bioc>
    <gcode>romp</gcode>
    <gcode>bugsparallel</gcode>
    <a href="http://slurm.schedmd.com/">Slurm open-source workload manager</a>
    <a href="http://www.cs.wisc.edu/condor/">Condor project at University of Wisconsin-Madison</a>
    <a href="http://www.imbi.uni-freiburg.de/parallel/">Parallel Computing in R with sfCluster/snowfall</a>
    <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">Wikipedia: Message Passing Interface (MPI)</a>
    <a href="http://en.wikipedia.org/wiki/Parallel_Virtual_Machine">Wikipedia: Parallel Virtual Machine (PVM)</a>
    <a href="http://dirk.eddelbuettel.com/papers/ismNov2009introHPCwithR.pdf">Slides from Introduction to High-Performance Computing with R tutorial help in Nov 2009 at the Institute for Statistical Mathematics, Tokyo, Japan</a> 
    <a href="https://gforge.nbic.nl/projects/rgpu/">rgpu project at nbic.nl</a>
    <a href="http://icl.cs.utk.edu/magma/">Magma: Matrix Algebra on GPU and Multicore architectures</a>
    <a href="http://shop.oreilly.com/product/0636920021421.do">Parallel R: Data Analysis in the Distributed World"</a>
    <a href="https://snoweye.github.io/hpsc/">High Performance Statistical Computing for Data Intensive Research</a>
    <a href="http://heather.cs.ucdavis.edu/~matloff/rth.html">Rth: Parallel R through Thrust</a>
    <a href="http://r-pbd.org">Programming with Big Data in R</a>
    <a href="https://github.com/saptarshiguha/RHIPE/">RHIPE</a>
    <!-- <a href="http://www.bayesian-inference.com/software">LaplacesDemon</a> -->
    <a href="https://github.com/ljdursi/beyond-single-core-R">Beyond Single Core: Parallel Analysis in R</a>
    <a href="https://github.com/eddelbuettel/ctv-hpc">GitHub repository for this Task View</a>
  </links>

</CRANTaskView>

